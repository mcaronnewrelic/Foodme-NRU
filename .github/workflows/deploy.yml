name: Deploy to EC2 with Terraform

"on":
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]
  workflow_dispatch:
    inputs:
      environment:
        description: "Environment to deploy to"
        required: true
        default: "staging"
        type: choice
        options:
          - staging
          - production

# Add explicit permissions for the workflow
permissions:
  contents: read
  actions: write
  deployments: write
  id-token: write

env:
  AWS_REGION: us-west-2
  TF_VERSION: 1.6.0
  NODE_VERSION: 22

jobs:
  build:
    name: Build Application
    runs-on: ubuntu-latest
    outputs:
      dist-artifact: ${{ steps.upload.outputs.artifact-id }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"

      - name: Install dependencies
        run: npm run install:all

      - name: Build complete application - also build Angular app
        run: npm run build

      - name: List dist contents
        run: |
          echo "Contents of dist directory:"
          ls -la dist/
          find dist/ -type f -name "*.js" -o -name "*.html" -o -name "*.css" -o -name "*.json" | head -20

      - name: Upload dist artifact
        id: upload
        uses: actions/upload-artifact@v4
        with:
          name: foodme-dist-${{ github.sha }}
          path: |
            dist/
            newrelic.js
            !dist/**/*.map
            !dist/**/test/**
          retention-days: 30

  terraform-plan:
    name: Terraform Plan
    runs-on: ubuntu-latest
    needs: build
    environment: ${{ github.event.inputs.environment || 'staging' }}
    outputs:
      tfplan: ${{ steps.plan.outputs.tfplan }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Terraform Format Check
        run: terraform fmt -check
        working-directory: ./terraform

      - name: Terraform Init
        run: terraform init
        working-directory: ./terraform
        env:
          TF_VAR_environment: ${{ github.event.inputs.environment || 'staging' }}

      - name: Terraform Validate
        run: terraform validate
        working-directory: ./terraform

      - name: Terraform Plan
        id: plan
        run: |
          terraform plan \
            -var="environment=${{ github.event.inputs.environment || 'staging' }}" \
            -var="app_version=${{ github.sha }}" \
            -out=tfplan
          echo "tfplan=tfplan" >> $GITHUB_OUTPUT
        working-directory: ./terraform
        env:
          TF_VAR_key_name: ${{ secrets.EC2_KEY_NAME }}
          TF_VAR_allowed_cidr_blocks: ${{ secrets.ALLOWED_CIDR_BLOCKS }}
          TF_VAR_new_relic_license_key: ${{ secrets.NEW_RELIC_LICENSE_KEY }}
          TF_VAR_db_name: ${{ secrets.DB_NAME || 'foodme' }}
          TF_VAR_db_user: ${{ secrets.DB_USER || 'foodme_user' }}
          TF_VAR_db_password: ${{ secrets.DB_PASSWORD || 'foodme_secure_password_2025!' }}
          TF_VAR_db_port: ${{ secrets.DB_PORT || '5432' }}

      - name: Upload Terraform Plan
        uses: actions/upload-artifact@v4
        with:
          name: terraform-plan-${{ github.sha }}
          path: terraform/tfplan
          retention-days: 5

  deploy:
    name: Deploy to EC2
    runs-on: ubuntu-latest
    needs: [build, terraform-plan]
    environment: ${{ github.event.inputs.environment || 'staging' }}
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master' || github.event_name == 'workflow_dispatch'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Download Terraform Plan
        uses: actions/download-artifact@v4
        with:
          name: terraform-plan-${{ github.sha }}
          path: terraform/

      - name: Terraform Init
        run: terraform init
        working-directory: ./terraform
        env:
          TF_VAR_environment: ${{ github.event.inputs.environment || 'staging' }}

      - name: Terraform Apply
        run: terraform apply -auto-approve tfplan
        working-directory: ./terraform
        env:
          TF_VAR_key_name: ${{ secrets.EC2_KEY_NAME }}
          TF_VAR_allowed_cidr_blocks: ${{ secrets.ALLOWED_CIDR_BLOCKS }}
          TF_VAR_new_relic_license_key: ${{ secrets.NEW_RELIC_LICENSE_KEY }}
          TF_VAR_db_name: ${{ secrets.DB_NAME || 'foodme' }}
          TF_VAR_db_user: ${{ secrets.DB_USER || 'foodme_user' }}
          TF_VAR_db_password: ${{ secrets.DB_PASSWORD || 'foodme_secure_password_2025!' }}
          TF_VAR_db_port: ${{ secrets.DB_PORT || '5432' }}

      - name: Get EC2 instance details
        id: instance
        run: |
          INSTANCE_IP=$(terraform output -raw instance_public_ip)
          INSTANCE_ID=$(terraform output -raw instance_id)
          echo "ip=$INSTANCE_IP" >> $GITHUB_OUTPUT
          echo "id=$INSTANCE_ID" >> $GITHUB_OUTPUT
        working-directory: ./terraform

      - name: Download dist artifact
        uses: actions/download-artifact@v4
        with:
          name: foodme-dist-${{ github.sha }}
          path: ./dist-download

      - name: List downloaded files for debugging
        run: |
          echo "üìÇ Contents of dist-download directory:"
          find ./dist-download -type f | head -20
          echo ""
          echo "üìÅ Directory structure:"
          ls -la ./dist-download/
          echo ""
          echo "üîç Looking for server files:"
          find ./dist-download -name "server" -type d || echo "No server directory found"
          find ./dist-download -name "*.js" -path "*/server/*" | head -10 || echo "No server JS files found"
          echo ""
          echo "üîç Looking for package.json:"
          find ./dist-download -name "package.json" || echo "No package.json found"
          echo ""
          echo "üîç Looking for frontend files:"
          find ./dist-download -name "*.html" -o -name "*.js" -o -name "*.css" | grep -v server | head -10 || echo "No frontend files found"

      - name: Check SSH key availability
        id: check_ssh
        run: |
          if [ -n "${{ secrets.EC2_PRIVATE_KEY }}" ]; then
            echo "ssh_available=true" >> $GITHUB_OUTPUT
          else
            echo "ssh_available=false" >> $GITHUB_OUTPUT
          fi

      - name: Setup SSH key
        if: steps.check_ssh.outputs.ssh_available == 'true'
        run: |
          # Create SSH directory
          mkdir -p ~/.ssh

          # Create private key file with proper formatting
          echo "${{ secrets.EC2_PRIVATE_KEY }}" > ~/.ssh/id_rsa

          # Verify the key was written correctly
          if [ ! -s ~/.ssh/id_rsa ]; then
            echo "‚ùå Failed to create SSH private key file"
            exit 1
          fi

          # Check key format
          if ! grep -q "BEGIN.*PRIVATE KEY" ~/.ssh/id_rsa; then
            echo "‚ùå Private key does not appear to be in correct format"
            echo "Expected format should start with '-----BEGIN PRIVATE KEY-----' or '-----BEGIN RSA PRIVATE KEY-----'"
            exit 1
          fi

          if ! grep -q "END.*PRIVATE KEY" ~/.ssh/id_rsa; then
            echo "‚ùå Private key does not appear to be complete"
            echo "Expected format should end with '-----END PRIVATE KEY-----' or '-----END RSA PRIVATE KEY-----'"
            exit 1
          fi

          # Set correct permissions
          chmod 600 ~/.ssh/id_rsa

          # Verify key file with ssh-keygen (with better error handling)
          if ! ssh-keygen -l -f ~/.ssh/id_rsa > /dev/null 2>&1; then
            echo "‚ùå SSH private key appears to be invalid or corrupted"
            echo "Key file size: $(wc -c < ~/.ssh/id_rsa) bytes"
            echo "Key file lines: $(wc -l < ~/.ssh/id_rsa) lines"
            exit 1
          fi

          echo "‚úÖ SSH key setup completed"
          echo "Key fingerprint: $(ssh-keygen -l -f ~/.ssh/id_rsa | cut -d' ' -f2)"

      - name: Wait for instance to be ready
        if: steps.check_ssh.outputs.ssh_available == 'true'
        run: |
          echo "Waiting for instance to be ready..."
          echo "Instance IP: ${{ steps.instance.outputs.ip }}"

          # Wait for SSH port to be open
          for i in {1..60}; do
            if nc -z ${{ steps.instance.outputs.ip }} 22 2>/dev/null; then
              echo "‚úÖ SSH port is open"
              break
            fi
            echo "‚è≥ Waiting for SSH (attempt $i/60)..."
            sleep 5
          done

          # Additional wait for services to start
          echo "Waiting additional 30 seconds for services to start..."
          sleep 30

      - name: Monitor cloud-init and user_data execution
        if: steps.check_ssh.outputs.ssh_available == 'true'
        timeout-minutes: 15
        continue-on-error: true
        run: |
          echo "üìã Monitoring user_data.sh script execution..."
          SSH_OPTS="-i ~/.ssh/id_rsa -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ConnectTimeout=10 -o ServerAliveInterval=10 -o ServerAliveCountMax=3"

          # Function to safely execute SSH commands with timeout
          ssh_exec() {
            timeout 60 ssh $SSH_OPTS ec2-user@${{ steps.instance.outputs.ip }} "$1" 2>/dev/null || echo "Command failed, timed out, or service not ready"
          }

          # Check if SSH is actually working first
          echo "üîç Testing SSH connectivity..."
          if ! timeout 30 ssh $SSH_OPTS ec2-user@${{ steps.instance.outputs.ip }} "echo 'SSH connected successfully'" 2>/dev/null; then
            echo "‚ùå SSH connection failed - cannot monitor cloud-init remotely"
            echo "üí° This could mean:"
            echo "   - Instance is still booting"
            echo "   - SSH key is incorrect"
            echo "   - Security group doesn't allow SSH"
            echo "   - Instance failed to start"
            echo ""
            echo "üîÑ Will continue deployment and check health endpoint instead..."
            exit 0
          fi
          echo "‚úÖ SSH connection successful"

          # Check cloud-init status with shorter timeout
          echo "üîç Checking cloud-init status..."
          if ssh_exec "cloud-init status --wait --long" | grep -q "done"; then
            echo "‚úÖ Cloud-init completed successfully"
          else
          else
            echo "‚è≥ Cloud-init still running or not responding, checking progress..."
            
            # Show last 30 lines of cloud-init output
            echo "üìú Recent cloud-init output:"
            ssh_exec "sudo tail -n 30 /var/log/cloud-init-output.log" || echo "Could not read cloud-init output"
            
            # Check if user_data script is running
            echo "üîç Checking if user_data script is running..."
            ssh_exec "ps aux | grep -v grep | grep -E '(user.?data|cloud.?init)'" || echo "No user_data processes found"
            
            # Wait a bit more and check again
            echo "‚è≥ Waiting 60 more seconds for cloud-init to complete..."
            sleep 60
            
            echo "üîç Final cloud-init status check..."
            ssh_exec "cloud-init status" || echo "Could not get final cloud-init status"
          fi

          # Display cloud-init logs
          echo ""
          echo "üìã ===== CLOUD-INIT OUTPUT LOG (last 100 lines) ====="
          ssh_exec "sudo tail -n 100 /var/log/cloud-init-output.log" || echo "Could not read cloud-init output log"

          echo ""
          echo "üìã ===== CUSTOM USER_DATA EXECUTION LOG ====="
          ssh_exec "sudo cat /var/log/user-data-execution.log 2>/dev/null" || echo "Custom log not found (script may still be running)"

          echo ""
          echo "üìã ===== CLOUD-INIT ERROR LOG (if any) ====="
          ssh_exec "sudo tail -n 20 /var/log/cloud-init.log | grep -i error" || echo "No errors found in cloud-init.log"

          echo ""
          echo "üìã ===== USER DATA SCRIPT SUMMARY ====="
          # Look for our specific markers in the output
          ssh_exec "sudo grep -E '(üöÄ|‚úÖ|‚ùå|‚ö†Ô∏è|üèÅ|üîÑ PROGRESS|PostgreSQL|New Relic|Nginx|EC2 setup completed)' /var/log/cloud-init-output.log /var/log/user-data-execution.log 2>/dev/null | tail -40" || echo "Could not find script markers"

          echo ""
          echo "üìã ===== SYSTEM STATUS AFTER USER_DATA ====="
          ssh_exec "sudo systemctl is-active postgresql-16 nginx newrelic-infra | xargs -I {} echo 'Service {}: {}'" || echo "Could not check service status"

          # Check if our config files were created
          echo ""
          echo "üìã ===== CONFIGURATION FILES STATUS ====="
          ssh_exec "ls -la /etc/nginx/conf.d/foodme.conf /etc/systemd/system/foodme.service /etc/newrelic-infra.yml 2>/dev/null | sed 's/^/  /' || echo 'Some config files missing'"

          echo ""
          echo "üìã ===== DOWNLOADED CONFIG FILES ====="
          ssh_exec "ls -la /home/ec2-user/foodme/config/ 2>/dev/null | sed 's/^/  /' || echo 'Config directory not found'"

          echo ""
          echo "‚úÖ Cloud-init monitoring completed"

      - name: Run diagnostic if monitoring times out
        if: steps.check_ssh.outputs.ssh_available == 'true' && failure()
        continue-on-error: true
        run: |
          echo "üö® Monitoring timed out - running diagnostic script..."
          SSH_OPTS="-i ~/.ssh/id_rsa -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ConnectTimeout=10"

          # Copy diagnostic script to instance
          if scp $SSH_OPTS ./terraform/debug-user-data.sh ec2-user@${{ steps.instance.outputs.ip }}:/tmp/debug-user-data.sh 2>/dev/null; then
            echo "üìã Running diagnostic script on instance..."
            ssh $SSH_OPTS ec2-user@${{ steps.instance.outputs.ip }} "chmod +x /tmp/debug-user-data.sh && sudo /tmp/debug-user-data.sh" 2>/dev/null || echo "Diagnostic script failed to run"
          else
            echo "‚ùå Could not copy diagnostic script to instance"
          fi

      - name: Deploy application
        if: steps.check_ssh.outputs.ssh_available == 'true'
        run: |
          # SSH options for better reliability
          SSH_OPTS="-i ~/.ssh/id_rsa -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ConnectTimeout=30 -o ServerAliveInterval=60"

          # Clean up any previous deployment files on server
          ssh $SSH_OPTS ec2-user@${{ steps.instance.outputs.ip }} "sudo rm -rf /tmp/foodme-deploy && mkdir -p /tmp/foodme-deploy"

          # Create deployment directory
          ssh $SSH_OPTS ec2-user@${{ steps.instance.outputs.ip }} "sudo mkdir -p /var/www/foodme"

          # Check what files we have to upload
          echo "üìÇ Files to upload:"
          if [ -d "./dist-download/dist" ]; then
            echo "‚úÖ Using dist-download/dist/ directory"
            find ./dist-download/dist -name "*.js" -o -name "*.html" -o -name "*.css" | head -10
            # Upload all files from the build artifact
            scp $SSH_OPTS -r ./dist-download/* ec2-user@${{ steps.instance.outputs.ip }}:/tmp/foodme-deploy/
          elif [ -d "./dist-download" ] && [ "$(find ./dist-download -name "*.js" -o -name "*.html" -o -name "*.css" | head -1)" ]; then
            echo "‚úÖ Using dist-download/ directory directly"
            find ./dist-download -name "*.js" -o -name "*.html" -o -name "*.css" | head -10
            # Upload files directly from dist-download
            scp $SSH_OPTS -r ./dist-download/* ec2-user@${{ steps.instance.outputs.ip }}:/tmp/foodme-deploy/
          else
            echo "‚ùå No web files found to upload"
            echo "Contents of current directory:"
            ls -la
            echo "Contents of dist-download:"
            ls -la ./dist-download/ || echo "dist-download directory not found"
            echo "Searching for any files in dist-download:"
            find ./dist-download -type f | head -20 || echo "No files found"
            exit 1
          fi

          # Upload database initialization files
          echo "üìÇ Uploading database initialization files..."
          if [ -d "./db" ]; then
            scp $SSH_OPTS -r ./db ec2-user@${{ steps.instance.outputs.ip }}:/tmp/foodme-deploy/
            echo "‚úÖ Database files uploaded"
          else
            echo "‚ö†Ô∏è No database directory found"
          fi

          # Move files to web directory and set permissions
          ssh $SSH_OPTS ec2-user@${{ steps.instance.outputs.ip }} "
            # Stop services first
            sudo systemctl stop foodme || true
            
            # Update systemd service environment variables using sed
            sudo sed -i 's/Environment=NEW_RELIC_LICENSE_KEY=.*/Environment=NEW_RELIC_LICENSE_KEY=${{ secrets.NEW_RELIC_LICENSE_KEY }}/' /etc/systemd/system/foodme.service
            sudo sed -i 's/Environment=NEW_RELIC_APP_NAME=.*/Environment=NEW_RELIC_APP_NAME=FoodMe-${{ github.event.inputs.environment || 'staging' }}/' /etc/systemd/system/foodme.service
            sudo sed -i 's/Environment=DB_PORT=.*/Environment=DB_PORT=${{ secrets.DB_PORT || '5432' }}/' /etc/systemd/system/foodme.service
            sudo sed -i 's/Environment=DB_NAME=.*/Environment=DB_NAME=${{ secrets.DB_NAME || 'foodme' }}/' /etc/systemd/system/foodme.service
            sudo sed -i 's/Environment=DB_USER=.*/Environment=DB_USER=${{ secrets.DB_USER || 'foodme_user' }}/' /etc/systemd/system/foodme.service
            sudo sed -i 's/Environment=DB_PASSWORD=.*/Environment=DB_PASSWORD=${{ secrets.DB_PASSWORD || 'foodme_secure_password_2025!' }}/' /etc/systemd/system/foodme.service
            
            # Reload systemd configuration after updating environment
            sudo systemctl daemon-reload
            
            # Verify the systemd service configuration
            echo 'Verifying systemd service environment variables:'
            grep 'Environment=NEW_RELIC' /etc/systemd/system/foodme.service || echo 'NEW_RELIC environment not found'
            grep 'Environment=DB_' /etc/systemd/system/foodme.service || echo 'DB environment not found'
            
            # Clear old deployment
            sudo rm -rf /var/www/foodme/dist /var/www/foodme/server /var/www/foodme/package*.json
            
            # Copy new files - ensure all application files are copied
            sudo cp -r /tmp/foodme-deploy/* /var/www/foodme/
            
            # Check if server directory exists in the deployed files
            if [ -d '/var/www/foodme/dist/server' ]; then
              echo '‚úÖ Server directory found in dist/server'
            else
              echo '‚ùå No server directory found! Listing contents:'
              find /var/www/foodme -type d -name '*server*' || echo 'No server directories found'
              ls -la /var/www/foodme/
              if [ -d '/var/www/foodme/dist' ]; then
                echo 'Contents of dist directory:'
                ls -la /var/www/foodme/dist/
              fi
            fi
            
            # Check if package.json exists
            if [ ! -f '/var/www/foodme/dist/server/package.json' ]; then
              echo '‚ùå package.json not found in /var/www/foodme/dist/server!'
              ls -la /var/www/foodme/dist/server
            else
              echo '‚úÖ package.json found'
            fi
            
            # Set proper permissions
            sudo chown -R ec2-user:ec2-user /var/www/foodme
            sudo chmod -R 755 /var/www/foodme
            
            # Run the deployment script
            if [ -x '/home/ec2-user/foodme/config/deploy.sh' ]; then
              echo 'Running deployment script...'
              cd /home/ec2-user/foodme/config && ./deploy.sh
            else
              echo 'No deployment script found, manually starting services...'
              
              # Install Node.js dependencies in server directory
              if [ -d '/var/www/foodme/dist/server' ] && [ -f '/var/www/foodme/server/package.json' ]; then
                echo 'Installing Node.js dependencies in server directory...'
                echo 'Node.js version:' && node --version
                echo 'npm version:' && npm --version
                echo 'Current directory:' && pwd
                echo 'Directory permissions:' && ls -la
                cd /var/www/foodme/server && npm install --production --unsafe-perm --verbose
              elif [ -f '/var/www/foodme/dist/package.json' ]; then
                echo 'Installing Node.js dependencies in root directory...'
                echo 'Node.js version:' && node --version
                echo 'npm version:' && npm --version
                echo 'Current directory:' && pwd
                echo 'Directory permissions:' && ls -la
                cd /var/www/foodme && npm install --production --unsafe-perm --verbose
              else
                echo 'No package.json found - dependencies may have been installed during build'
              fi
              
              # Start services
              sudo systemctl start foodme
              
              # Verify environment variables are available to the service
              echo 'Checking if New Relic environment is accessible to the service:'
              sudo systemctl show foodme --property=Environment | grep NEW_RELIC || echo 'NEW_RELIC not found in service environment'
              
              # Check service status
              echo 'Service status:'
              sudo systemctl is-active foodme || echo 'foodme failed to start'
              
              # Show detailed status if foodme failed
              if ! sudo systemctl is-active foodme >/dev/null; then
                echo 'Foodme service failed - showing detailed status:'
                sudo systemctl status foodme --no-pager || true
                echo 'Recent logs:'
                sudo journalctl -u foodme -n 20 --no-pager || true
                
                # Additional debugging for server directory
                echo 'Checking server directory structure:'
                ls -la /var/www/foodme/dist/server/ || echo 'Server directory not found'
                echo 'Checking if start.js exists:'
                ls -la /var/www/foodme/dist/dist/server/start.js || echo 'start.js not found'
                echo 'Checking package.json in server:'
                ls -la /var/www/foodme/server/package.json || echo 'package.json not found in server'
              fi
            fi
            
            # Clean up temp files
            rm -rf /tmp/foodme-deploy
          "

      - name: Alternative deployment notice
        if: steps.check_ssh.outputs.ssh_available == 'false'
        run: |
          echo "üîÑ SSH deployment skipped - no EC2_PRIVATE_KEY provided"
          echo "üìù To enable SSH deployment:"
          echo "   1. Run: cd terraform && ./create-keypair.sh"
          echo "   2. Add EC2_KEY_NAME and EC2_PRIVATE_KEY secrets to GitHub"
          echo "   3. Re-run the workflow"
          echo ""
          echo "üí° Alternative: Use AWS Systems Manager Session Manager for deployment"
          echo "   This would require modifying the deployment strategy"
          echo ""
          echo "üìã To manually check user_data.sh script execution:"
          echo "   1. Connect to EC2 instance: aws ssm start-session --target ${{ steps.instance.outputs.id }}"
          echo "   2. Check cloud-init status: cloud-init status --wait"
          echo "   3. View logs: sudo cat /var/log/cloud-init-output.log"
          echo "   4. Check errors: sudo tail -f /var/log/cloud-init.log"

      - name: Monitor cloud-init without SSH (using AWS CLI)
        if: steps.check_ssh.outputs.ssh_available == 'false'
        run: |
          echo "üìã Attempting to get cloud-init status via AWS Systems Manager..."
          INSTANCE_ID="${{ steps.instance.outputs.id }}"

          # Wait for the instance to be ready for SSM
          echo "‚è≥ Waiting for instance to be ready for AWS Systems Manager..."
          aws ec2 wait instance-status-ok --instance-ids $INSTANCE_ID || echo "Instance status check timed out"

          # Try to run commands via SSM (this requires the instance to have SSM agent and proper IAM role)
          echo "üîç Attempting to check cloud-init status via SSM..."
          COMMAND_ID=$(aws ssm send-command \
            --instance-ids $INSTANCE_ID \
            --document-name "AWS-RunShellScript" \
            --parameters 'commands=["cloud-init status --wait --long", "sudo tail -n 20 /var/log/cloud-init-output.log"]' \
            --query 'Command.CommandId' \
            --output text 2>/dev/null || echo "failed")

          if [ "$COMMAND_ID" != "failed" ]; then
            echo "üìã Command ID: $COMMAND_ID"
            echo "‚è≥ Waiting for command to complete..."
            sleep 30
            
            echo "üìã Command output:"
            aws ssm get-command-invocation \
              --command-id $COMMAND_ID \
              --instance-id $INSTANCE_ID \
              --query 'StandardOutputContent' \
              --output text 2>/dev/null || echo "Could not retrieve command output"
              
            echo "üìã Command errors (if any):"
            aws ssm get-command-invocation \
              --command-id $COMMAND_ID \
              --instance-id $INSTANCE_ID \
              --query 'StandardErrorContent' \
              --output text 2>/dev/null || echo "Could not retrieve command errors"
          else
            echo "‚ùå Could not execute SSM commands. Instance may not have SSM agent or proper IAM role."
            echo "üí° Manual steps to check user_data execution:"
            echo "   Instance ID: $INSTANCE_ID"
            echo "   1. Go to AWS Console ‚Üí EC2 ‚Üí Instances"
            echo "   2. Select instance $INSTANCE_ID"
            echo "   3. Actions ‚Üí Monitor and troubleshoot ‚Üí Get system log"
            echo "   4. Or use Session Manager to connect and run:"
            echo "      sudo cat /var/log/cloud-init-output.log"
          fi

      - name: Health check
        id: health_check
        run: |
          echo "Performing health check..."
          echo "Instance IP: ${{ steps.instance.outputs.ip }}"

          # Wait a bit for services to start if SSH deployment was skipped
          if [ "${{ steps.check_ssh.outputs.ssh_available }}" == "false" ]; then
            echo "‚ÑπÔ∏è  SSH deployment was skipped, waiting longer for user_data script to complete..."
            echo "‚è≥ Waiting 3 minutes for user_data script and services to start..."
            sleep 180
          else
            echo "‚ÑπÔ∏è  SSH deployment completed, waiting for services to stabilize..."
            sleep 60
          fi

          # First, check if the instance is reachable
          echo "üîç Testing basic connectivity..."
          if ! curl -s --connect-timeout 10 http://${{ steps.instance.outputs.ip }} > /dev/null; then
            echo "‚ùå Cannot connect to instance - checking if it's still starting up..."
          fi

          # Check different endpoints to diagnose the issue
          echo "üîç Performing comprehensive health checks..."

          # Track consecutive 502 errors for targeted intervention
          consecutive_502_count=0

          for i in {1..15}; do
            echo "‚è≥ Health check attempt $i/15..."
            
            # Try the health endpoint first
            HEALTH_CODE=$(curl -s -o /dev/null -w "%{http_code}" --connect-timeout 10 --max-time 30 http://${{ steps.instance.outputs.ip }}/health 2>/dev/null || echo "000")
            
            if [ "$HEALTH_CODE" = "200" ]; then
              echo "‚úÖ Health check passed on /health endpoint"
              echo "health_passed=true" >> $GITHUB_OUTPUT
              exit 0
            fi
            
            # Try the root endpoint as fallback
            ROOT_CODE=$(curl -s -o /dev/null -w "%{http_code}" --connect-timeout 10 --max-time 30 http://${{ steps.instance.outputs.ip }}/ 2>/dev/null || echo "000")
            
            if [ "$ROOT_CODE" = "200" ]; then
              echo "‚úÖ Root endpoint accessible - application seems to be running"
              echo "health_passed=true" >> $GITHUB_OUTPUT
              exit 0
            fi
            
            # Analyze the error codes
            echo "üìä Health endpoint: HTTP $HEALTH_CODE, Root endpoint: HTTP $ROOT_CODE"
            
            # Handle specific error scenarios
            if [ "$HEALTH_CODE" = "502" ] || [ "$ROOT_CODE" = "502" ]; then
              consecutive_502_count=$((consecutive_502_count + 1))
              echo "üîç 502 Bad Gateway (count: $consecutive_502_count) - nginx running but backend not responding"
              
              # If we've seen multiple 502s and have SSH access, try to restart the service
              if [ $consecutive_502_count -ge 3 ] && [ "${{ steps.check_ssh.outputs.ssh_available }}" == "true" ] && [ $i -le 10 ]; then
                echo "üîß Attempting to restart backend service via SSH..."
                
                # Create a temporary script for service restart
                {
                  echo '#!/bin/bash'
                  echo 'echo "Checking service status..."'
                  echo 'sudo systemctl status foodme --no-pager || true'
                  echo ''
                  echo 'echo "Checking if Node.js process is running..."'
                  echo 'ps aux | grep node || true'
                  echo ''
                  echo 'echo "Checking if port 3000 is in use..."'
                  echo 'sudo netstat -tlnp | grep :3000 || true'
                  echo ''
                  echo 'echo "Restarting foodme service..."'
                  echo 'sudo systemctl restart foodme'
                  echo ''
                  echo 'echo "Waiting for service to start..."'
                  echo 'sleep 10'
                  echo ''
                  echo 'echo "Checking service status after restart..."'
                  echo 'sudo systemctl status foodme --no-pager || true'
                  echo ''
                  echo 'echo "Testing local health endpoint..."'
                  echo 'curl -f http://localhost:3000/health || curl -s http://localhost:3000/health || echo "Local health check failed"'
                } > restart_service.sh

                # Execute the restart script
                if scp -i /tmp/ssh_key -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null restart_service.sh ec2-user@${{ steps.instance.outputs.ip }}:/tmp/restart_service.sh; then
                  ssh -i /tmp/ssh_key -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ec2-user@${{ steps.instance.outputs.ip }} "chmod +x /tmp/restart_service.sh && /tmp/restart_service.sh" || true
                  echo "üîß Service restart attempted, waiting 30 seconds..."
                  sleep 30
                  consecutive_502_count=0  # Reset counter after intervention
                  continue
                else
                  echo "‚ö†Ô∏è  Could not restart service via SSH"
                fi
              fi
              
            elif [ "$HEALTH_CODE" = "503" ] || [ "$ROOT_CODE" = "503" ]; then
              echo "üîç 503 Service Unavailable - service temporarily unavailable"
              consecutive_502_count=0  # Reset 502 counter
              
            elif [ "$HEALTH_CODE" = "000" ] && [ "$ROOT_CODE" = "000" ]; then
              echo "üîç Connection failed - instance may still be starting or network issue"
              consecutive_502_count=0  # Reset 502 counter
              
            elif [ "$HEALTH_CODE" = "404" ] || [ "$ROOT_CODE" = "404" ]; then
              echo "üîç 404 Not Found - nginx running but routes not configured properly"
              consecutive_502_count=0  # Reset 502 counter
              
            else
              echo "üîç Unexpected response codes"
              consecutive_502_count=0  # Reset 502 counter
            fi
            
            # Provide specific guidance based on error pattern
            if [ $consecutive_502_count -ge 2 ]; then
              echo "üîç Consistent 502 errors suggest backend service issues:"
              echo "   - Node.js app may have crashed or failed to start"
              echo "   - Dependencies might be missing"
              echo "   - Port 3000 may not be listening"
            fi
            
            # Wait before next attempt (longer wait for 502 errors)
            if [ $i -lt 15 ]; then
              if [ "$HEALTH_CODE" = "502" ] || [ "$ROOT_CODE" = "502" ]; then
                echo "‚è≥ 502 error detected, waiting 45 seconds for potential auto-recovery..."
                sleep 45
              else
                echo "‚è≥ Waiting 30 seconds before next attempt..."
                sleep 30
              fi
            fi
          done

          echo "‚ùå Health check failed after 15 attempts"
          echo "health_passed=false" >> $GITHUB_OUTPUT

          # Final diagnostic information
          echo ""
          echo "ÔøΩ Final diagnostic check:"
          echo "Instance IP: ${{ steps.instance.outputs.ip }}"

          # Check if nginx is responding at all
          if curl -s --connect-timeout 5 http://${{ steps.instance.outputs.ip }} > /dev/null 2>&1; then
            echo "‚úÖ Nginx is responding"
          else
            echo "‚ùå Cannot connect to nginx"
          fi

          echo ""
          echo "üí° Troubleshooting steps:"
          echo "1. SSH into the instance: ssh -i your-key.pem ec2-user@${{ steps.instance.outputs.ip }}"
          echo "2. Check service status: sudo systemctl status nginx foodme"
          echo "3. Check logs: sudo journalctl -u foodme -n 50"
          echo "4. Check nginx logs: sudo tail -f /var/log/nginx/error.log"
          echo "5. Check if app is listening: sudo netstat -tlnp | grep :3000"

          exit 1
      - name: Deployment failure cleanup
        if: failure() && steps.health_check.outputs.health_passed == 'false'
        run: |
          echo "üö® Deployment failed during health check. Consider cleaning up AWS resources."
          echo "health_check_failed=true" >> $GITHUB_ENV
        continue-on-error: true

      - name: Set Release Version from Tag
        run: echo "RELEASE_VERSION=${{ github.ref_name }}" >> $GITHUB_ENV
      - name: New Relic Application Deployment Marker
        uses: newrelic/deployment-marker-action@v2.5.1
        with:
          guid: ${{ secrets.NEW_RELIC_APP_ID }}
          apiKey: ${{ secrets.NEW_RELIC_API_KEY }}
          region: "US"
          user: ${{ github.actor }}
          commit: ${{ github.sha }}
          changelog: ${{ github.event.head_commit.message }}
          version: "${{ env.RELEASE_VERSION }}"
          description: "Deployed FoodMe-${{ github.event.inputs.environment || 'staging' }} by ${{ github.actor }} to ${{ github.event.inputs.environment || 'staging' }}"

      - name: Deployment summary
        run: |
          echo "## üöÄ Deployment Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Environment**: ${{ github.event.inputs.environment || 'staging' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Instance ID**: ${{ steps.instance.outputs.id }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Instance IP**: ${{ steps.instance.outputs.ip }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Application URL**: http://${{ steps.instance.outputs.ip }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Deployed by**: ${{ github.actor }}" >> $GITHUB_STEP_SUMMARY

  cleanup-on-failure:
    name: Cleanup AWS Resources on Failure
    runs-on: ubuntu-latest
    needs: [deploy]
    if: failure() && needs.deploy.result == 'failure'
    environment: ${{ github.event.inputs.environment || 'staging' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Terraform Init
        run: terraform init
        working-directory: ./terraform
        continue-on-error: true

      - name: Terraform Destroy
        run: |
          echo "üßπ Cleaning up AWS resources due to deployment failure..."
          terraform destroy -auto-approve \
            -var="environment=${{ github.event.inputs.environment || 'staging' }}" \
            -var="app_version=${{ github.sha }}" || echo "‚ö†Ô∏è Some resources may have failed to destroy"
        working-directory: ./terraform
        env:
          TF_VAR_key_name: ${{ secrets.EC2_KEY_NAME }}
          TF_VAR_allowed_cidr_blocks: ${{ secrets.ALLOWED_CIDR_BLOCKS }}
        continue-on-error: true

      - name: Cleanup summary
        run: |
          echo "## üßπ Cleanup Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Reason**: Deployment failed, cleaned up AWS resources" >> $GITHUB_STEP_SUMMARY
          echo "- **Environment**: ${{ github.event.inputs.environment || 'staging' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Action**: Terraform destroy executed" >> $GITHUB_STEP_SUMMARY
          echo "- **Note**: Please verify in AWS console that all resources were removed" >> $GITHUB_STEP_SUMMARY

  cleanup-artifacts:
    name: Cleanup Build Artifacts
    runs-on: ubuntu-latest
    needs: [deploy, cleanup-on-failure]
    if: always()

    steps:
      - name: Delete artifacts
        uses: actions/github-script@v7
        with:
          script: |
            try {
              const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
                owner: context.repo.owner,
                repo: context.repo.repo,
                run_id: context.runId,
              });

              let deletedCount = 0;
              let failedCount = 0;

              for (const artifact of artifacts.data.artifacts) {
                if (artifact.name.includes('${{ github.sha }}')) {
                  try {
                    await github.rest.actions.deleteArtifact({
                      owner: context.repo.owner,
                      repo: context.repo.repo,
                      artifact_id: artifact.id,
                    });
                    console.log(`‚úÖ Deleted artifact: ${artifact.name}`);
                    deletedCount++;
                  } catch (error) {
                    console.log(`‚ö†Ô∏è Failed to delete artifact ${artifact.name}: ${error.message}`);
                    failedCount++;
                  }
                }
              }

              console.log(`\nüìä Cleanup Summary:`);
              console.log(`- Artifacts deleted: ${deletedCount}`);
              console.log(`- Deletion failures: ${failedCount}`);
              
              if (failedCount > 0) {
                console.log(`\nüí° Note: Some artifacts may be automatically cleaned up by GitHub after the retention period.`);
                console.log(`This error is usually due to permission restrictions and won't affect your application.`);
              }
            } catch (error) {
              console.log(`‚ö†Ô∏è Unable to cleanup artifacts: ${error.message}`);
              console.log(`This is usually due to GitHub token permissions and won't affect your application.`);
              console.log(`Artifacts will be automatically cleaned up by GitHub after the retention period.`);
            }
        continue-on-error: true
