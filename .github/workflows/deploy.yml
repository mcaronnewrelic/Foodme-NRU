name: Deploy to EC2 with Terraform

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]
  workflow_dispatch:
    inputs:
      environment:
        description: "Environment to deploy to"
        required: true
        default: "staging"
        type: choice
        options:
          - staging
          - production

env:
  AWS_REGION: us-west-2
  TF_VERSION: 1.6.0
  NODE_VERSION: 22

jobs:
  build:
    name: Build Application
    runs-on: ubuntu-latest
    outputs:
      dist-artifact: ${{ steps.upload.outputs.artifact-id }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"

      - name: Install dependencies
        run: npm run install:all

      - name: Build Angular app
        run: npm run build:angular

      - name: Build complete application
        run: npm run build

      - name: List dist contents
        run: |
          echo "Contents of dist directory:"
          ls -la dist/
          find dist/ -type f -name "*.js" -o -name "*.html" -o -name "*.css" | head -20

      - name: Upload dist artifact
        id: upload
        uses: actions/upload-artifact@v4
        with:
          name: foodme-dist-${{ github.sha }}
          path: |
            dist/
            !dist/**/*.map
            !dist/**/test/**
          retention-days: 30

  terraform-plan:
    name: Terraform Plan
    runs-on: ubuntu-latest
    needs: build
    environment: ${{ github.event.inputs.environment || 'staging' }}
    outputs:
      tfplan: ${{ steps.plan.outputs.tfplan }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Terraform Format Check
        run: terraform fmt -check
        working-directory: ./terraform

      - name: Terraform Init
        run: terraform init
        working-directory: ./terraform
        env:
          TF_VAR_environment: ${{ github.event.inputs.environment || 'staging' }}

      - name: Terraform Validate
        run: terraform validate
        working-directory: ./terraform

      - name: Terraform Plan
        id: plan
        run: |
          terraform plan \
            -var="environment=${{ github.event.inputs.environment || 'staging' }}" \
            -var="app_version=${{ github.sha }}" \
            -out=tfplan
          echo "tfplan=tfplan" >> $GITHUB_OUTPUT
        working-directory: ./terraform
        env:
          TF_VAR_key_name: ${{ secrets.EC2_KEY_NAME }}
          TF_VAR_allowed_cidr_blocks: ${{ secrets.ALLOWED_CIDR_BLOCKS }}

      - name: Upload Terraform Plan
        uses: actions/upload-artifact@v4
        with:
          name: terraform-plan-${{ github.sha }}
          path: terraform/tfplan
          retention-days: 5

  deploy:
    name: Deploy to EC2
    runs-on: ubuntu-latest
    needs: [build, terraform-plan]
    environment: ${{ github.event.inputs.environment || 'staging' }}
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master' || github.event_name == 'workflow_dispatch'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Download Terraform Plan
        uses: actions/download-artifact@v4
        with:
          name: terraform-plan-${{ github.sha }}
          path: terraform/

      - name: Terraform Init
        run: terraform init
        working-directory: ./terraform
        env:
          TF_VAR_environment: ${{ github.event.inputs.environment || 'staging' }}

      - name: Terraform Apply
        run: terraform apply -auto-approve tfplan
        working-directory: ./terraform
        env:
          TF_VAR_key_name: ${{ secrets.EC2_KEY_NAME }}
          TF_VAR_allowed_cidr_blocks: ${{ secrets.ALLOWED_CIDR_BLOCKS }}

      - name: Get EC2 instance details
        id: instance
        run: |
          INSTANCE_IP=$(terraform output -raw instance_public_ip)
          INSTANCE_ID=$(terraform output -raw instance_id)
          echo "ip=$INSTANCE_IP" >> $GITHUB_OUTPUT
          echo "id=$INSTANCE_ID" >> $GITHUB_OUTPUT
        working-directory: ./terraform

      - name: Download dist artifact
        uses: actions/download-artifact@v4
        with:
          name: foodme-dist-${{ github.sha }}
          path: ./dist-download

      - name: List downloaded files for debugging
        run: |
          echo "üìÇ Contents of dist-download directory:"
          find ./dist-download -type f | head -20
          echo ""
          echo "üìÅ Directory structure:"
          ls -la ./dist-download/

      - name: Check SSH key availability
        id: check_ssh
        run: |
          if [ -n "${{ secrets.EC2_PRIVATE_KEY }}" ]; then
            echo "ssh_available=true" >> $GITHUB_OUTPUT
          else
            echo "ssh_available=false" >> $GITHUB_OUTPUT
          fi

      - name: Setup SSH key
        if: steps.check_ssh.outputs.ssh_available == 'true'
        run: |
          # Create SSH directory
          mkdir -p ~/.ssh
          
          # Create private key file with proper formatting
          echo "${{ secrets.EC2_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          
          # Verify the key was written correctly
          if [ ! -s ~/.ssh/id_rsa ]; then
            echo "‚ùå Failed to create SSH private key file"
            exit 1
          fi
          
          # Check key format
          if ! grep -q "BEGIN.*PRIVATE KEY" ~/.ssh/id_rsa; then
            echo "‚ùå Private key does not appear to be in correct format"
            echo "Expected format should start with '-----BEGIN PRIVATE KEY-----' or '-----BEGIN RSA PRIVATE KEY-----'"
            exit 1
          fi
          
          if ! grep -q "END.*PRIVATE KEY" ~/.ssh/id_rsa; then
            echo "‚ùå Private key does not appear to be complete"
            echo "Expected format should end with '-----END PRIVATE KEY-----' or '-----END RSA PRIVATE KEY-----'"
            exit 1
          fi
          
          # Set correct permissions
          chmod 600 ~/.ssh/id_rsa
          
          # Verify key file with ssh-keygen (with better error handling)
          if ! ssh-keygen -l -f ~/.ssh/id_rsa > /dev/null 2>&1; then
            echo "‚ùå SSH private key appears to be invalid or corrupted"
            echo "Key file size: $(wc -c < ~/.ssh/id_rsa) bytes"
            echo "Key file lines: $(wc -l < ~/.ssh/id_rsa) lines"
            exit 1
          fi
          
          echo "‚úÖ SSH key setup completed"
          echo "Key fingerprint: $(ssh-keygen -l -f ~/.ssh/id_rsa | cut -d' ' -f2)"

      - name: Wait for instance to be ready
        if: steps.check_ssh.outputs.ssh_available == 'true'
        run: |
          echo "Waiting for instance to be ready..."
          echo "Instance IP: ${{ steps.instance.outputs.ip }}"
          
          # Wait for SSH port to be open
          for i in {1..60}; do
            if nc -z ${{ steps.instance.outputs.ip }} 22 2>/dev/null; then
              echo "‚úÖ SSH port is open"
              break
            fi
            echo "‚è≥ Waiting for SSH (attempt $i/60)..."
            sleep 5
          done
          
          # Additional wait for services to start
          echo "Waiting additional 30 seconds for services to start..."
          sleep 30

      - name: Deploy application
        if: steps.check_ssh.outputs.ssh_available == 'true'
        run: |
          # SSH options for better reliability
          SSH_OPTS="-i ~/.ssh/id_rsa -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ConnectTimeout=30 -o ServerAliveInterval=60"
          
          # Clean up any previous deployment files on server
          ssh $SSH_OPTS ec2-user@${{ steps.instance.outputs.ip }} "sudo rm -rf /tmp/foodme-deploy && mkdir -p /tmp/foodme-deploy"
          
          # Create deployment directory
          ssh $SSH_OPTS ec2-user@${{ steps.instance.outputs.ip }} "sudo mkdir -p /var/www/foodme"

          # Check what files we have to upload
          echo "üìÇ Files to upload:"
          if [ -d "./dist-download/dist" ]; then
            echo "‚úÖ Using dist-download/dist/ directory"
            find ./dist-download/dist -name "*.js" -o -name "*.html" -o -name "*.css" | head -10
            # Upload dist files from the correct path
            scp $SSH_OPTS -r ./dist-download/dist/* ec2-user@${{ steps.instance.outputs.ip }}:/tmp/foodme-deploy/
          elif [ -d "./dist-download" ] && [ "$(find ./dist-download -name "*.js" -o -name "*.html" -o -name "*.css" | head -1)" ]; then
            echo "‚úÖ Using dist-download/ directory directly"
            find ./dist-download -name "*.js" -o -name "*.html" -o -name "*.css" | head -10
            # Upload files directly from dist-download
            scp $SSH_OPTS -r ./dist-download/* ec2-user@${{ steps.instance.outputs.ip }}:/tmp/foodme-deploy/
          else
            echo "‚ùå No web files found to upload"
            echo "Contents of current directory:"
            ls -la
            echo "Contents of dist-download:"
            ls -la ./dist-download/ || echo "dist-download directory not found"
            echo "Searching for any files in dist-download:"
            find ./dist-download -type f | head -20 || echo "No files found"
            exit 1
          fi

          # Move files to web directory and set permissions
          ssh $SSH_OPTS ec2-user@${{ steps.instance.outputs.ip }} "
            # Stop services first
            sudo systemctl stop foodme || true
            sudo systemctl stop nginx || true
            
            # Clear old deployment
            sudo rm -rf /var/www/foodme/*
            
            # Copy new files
            sudo cp -r /tmp/foodme-deploy/* /var/www/foodme/
            
            # Set proper permissions
            sudo chown -R nginx:nginx /var/www/foodme
            sudo chmod -R 755 /var/www/foodme
            
            # Create logs directory if it doesn't exist
            sudo mkdir -p /var/log/foodme
            sudo chown ec2-user:ec2-user /var/log/foodme
            
            # Start services
            sudo systemctl start nginx
            sudo systemctl start foodme
            
            # Check service status
            echo 'Service status:'
            sudo systemctl is-active nginx || echo 'nginx failed to start'
            sudo systemctl is-active foodme || echo 'foodme failed to start'
            
            # Clean up temp files
            rm -rf /tmp/foodme-deploy
          "

      - name: Alternative deployment notice
        if: steps.check_ssh.outputs.ssh_available == 'false'
        run: |
          echo "üîÑ SSH deployment skipped - no EC2_PRIVATE_KEY provided"
          echo "üìù To enable SSH deployment:"
          echo "   1. Run: cd terraform && ./create-keypair.sh"
          echo "   2. Add EC2_KEY_NAME and EC2_PRIVATE_KEY secrets to GitHub"
          echo "   3. Re-run the workflow"
          echo ""
          echo "üí° Alternative: Use AWS Systems Manager Session Manager for deployment"
          echo "   This would require modifying the deployment strategy"

      - name: Health check
        id: health_check
        run: |
          echo "Performing health check..."
          echo "Instance IP: ${{ steps.instance.outputs.ip }}"

          # Wait a bit for services to start if SSH deployment was skipped
          if [ "${{ steps.check_ssh.outputs.ssh_available }}" == "false" ]; then
            echo "‚ÑπÔ∏è  SSH deployment was skipped, waiting longer for user_data script to complete..."
            sleep 120
          fi

          for i in {1..10}; do
            if curl -f http://${{ steps.instance.outputs.ip }}/health; then
              echo "‚úÖ Health check passed"
              echo "health_passed=true" >> $GITHUB_OUTPUT
              exit 0
            fi
            echo "‚è≥ Attempt $i failed, waiting 30 seconds..."
            sleep 30
          done
          echo "‚ùå Health check failed"
          echo "health_passed=false" >> $GITHUB_OUTPUT
          echo "üí° Check the EC2 instance logs or SSH into the instance to debug"
          exit 1

      - name: Deployment failure cleanup
        if: failure() && steps.health_check.outputs.health_passed == 'false'
        run: |
          echo "üö® Deployment failed during health check. Consider cleaning up AWS resources."
          echo "health_check_failed=true" >> $GITHUB_ENV
        continue-on-error: true

      - name: Send New Relic deployment marker
        if: success()
        run: |
          USER="${{ github.actor }}"
          CHANGELOG="${{ github.event.head_commit.message }}"
          COMMIT="${{ github.sha }}"

          curl -X POST https://api.newrelic.com/graphql \
            -H 'Content-Type: application/json' \
            -H 'API-Key: ${{ secrets.NEW_RELIC_API_KEY }}' \
            --data-raw '{
              "query": "mutation { 
                changeTrackingCreateDeployment(deployment: {
                  description: \"FoodMe EC2 Deployment via GitHub Actions\",
                  user: \"'$USER'\",
                  changelog: \"'$CHANGELOG'\",
                  commit: \"'$COMMIT'\",
                  deploymentType: BASIC
                }) { 
                  deploymentId 
                } 
              }"
            }'
        continue-on-error: true

      - name: Deployment summary
        run: |
          echo "## üöÄ Deployment Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Environment**: ${{ github.event.inputs.environment || 'staging' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Instance ID**: ${{ steps.instance.outputs.id }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Instance IP**: ${{ steps.instance.outputs.ip }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Application URL**: http://${{ steps.instance.outputs.ip }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Deployed by**: ${{ github.actor }}" >> $GITHUB_STEP_SUMMARY

  cleanup-on-failure:
    name: Cleanup AWS Resources on Failure
    runs-on: ubuntu-latest
    needs: [deploy]
    if: failure() && needs.deploy.result == 'failure'
    environment: ${{ github.event.inputs.environment || 'staging' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Terraform Init
        run: terraform init
        working-directory: ./terraform
        continue-on-error: true

      - name: Terraform Destroy
        run: |
          echo "üßπ Cleaning up AWS resources due to deployment failure..."
          terraform destroy -auto-approve \
            -var="environment=${{ github.event.inputs.environment || 'staging' }}" \
            -var="app_version=${{ github.sha }}" || echo "‚ö†Ô∏è Some resources may have failed to destroy"
        working-directory: ./terraform
        env:
          TF_VAR_key_name: ${{ secrets.EC2_KEY_NAME }}
          TF_VAR_allowed_cidr_blocks: ${{ secrets.ALLOWED_CIDR_BLOCKS }}
        continue-on-error: true

      - name: Cleanup summary
        run: |
          echo "## üßπ Cleanup Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Reason**: Deployment failed, cleaned up AWS resources" >> $GITHUB_STEP_SUMMARY
          echo "- **Environment**: ${{ github.event.inputs.environment || 'staging' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Action**: Terraform destroy executed" >> $GITHUB_STEP_SUMMARY
          echo "- **Note**: Please verify in AWS console that all resources were removed" >> $GITHUB_STEP_SUMMARY

  cleanup-artifacts:
    name: Cleanup Build Artifacts
    runs-on: ubuntu-latest
    needs: [deploy, cleanup-on-failure]
    if: always()

    steps:
      - name: Delete artifacts
        uses: actions/github-script@v7
        with:
          script: |
            const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
              owner: context.repo.owner,
              repo: context.repo.repo,
              run_id: context.runId,
            });

            for (const artifact of artifacts.data.artifacts) {
              if (artifact.name.includes('${{ github.sha }}')) {
                await github.rest.actions.deleteArtifact({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  artifact_id: artifact.id,
                });
                console.log(`Deleted artifact: ${artifact.name}`);
              }
            }
