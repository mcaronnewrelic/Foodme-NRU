name: Deploy to EC2 with Terraform

"on":
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]
  workflow_dispatch:
    inputs:
      environment:
        description: "Environment to deploy to"
        required: true
        default: "staging"
        type: choice
        options:
          - staging
          - production

# Add explicit permissions for the workflow
permissions:
  contents: read
  actions: write
  deployments: write
  id-token: write

env:
  AWS_REGION: us-west-2
  TF_VERSION: 1.6.0
  NODE_VERSION: 22

jobs:
  build:
    name: Build Application
    runs-on: ubuntu-latest
    outputs:
      dist-artifact: ${{ steps.upload.outputs.artifact-id }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"

      - name: Install dependencies
        run: npm run install:all

      - name: Build complete application - also build Angular app
        run: npm run build

      - name: List dist contents
        run: |
          echo "Contents of dist directory:"
          ls -la dist/
          find dist/ -type f -name "*.js" -o -name "*.html" -o -name "*.css" -o -name "*.json" | head -20

      - name: Upload dist artifact
        id: upload
        uses: actions/upload-artifact@v4
        with:
          name: foodme-dist-${{ github.sha }}
          path: |
            dist/
            newrelic.js
            !dist/**/*.map
            !dist/**/test/**
          retention-days: 30

  terraform-plan:
    name: Terraform Plan
    runs-on: ubuntu-latest
    needs: build
    environment: ${{ github.event.inputs.environment || 'staging' }}
    outputs:
      tfplan: ${{ steps.plan.outputs.tfplan }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Terraform Format Check
        run: terraform fmt -check
        working-directory: ./terraform

      - name: Terraform Init
        run: terraform init
        working-directory: ./terraform
        env:
          TF_VAR_environment: ${{ github.event.inputs.environment || 'staging' }}

      - name: Terraform Validate
        run: terraform validate
        working-directory: ./terraform

      - name: Terraform Plan
        id: plan
        run: |
          terraform plan \
            -var="environment=${{ github.event.inputs.environment || 'staging' }}" \
            -var="app_version=${{ github.sha }}" \
            -out=tfplan
          echo "tfplan=tfplan" >> $GITHUB_OUTPUT
        working-directory: ./terraform
        env:
          TF_VAR_key_name: ${{ secrets.EC2_KEY_NAME }}
          TF_VAR_allowed_cidr_blocks: ${{ secrets.ALLOWED_CIDR_BLOCKS }}
          TF_VAR_new_relic_license_key: ${{ secrets.NEW_RELIC_LICENSE_KEY }}
          TF_VAR_db_name: ${{ secrets.DB_NAME || 'foodme' }}
          TF_VAR_db_user: ${{ secrets.DB_USER || 'foodme_user' }}
          TF_VAR_db_password: ${{ secrets.DB_PASSWORD || 'foodme_secure_password_2025!' }}
          TF_VAR_db_port: ${{ secrets.DB_PORT || '5432' }}

      - name: Upload Terraform Plan
        uses: actions/upload-artifact@v4
        with:
          name: terraform-plan-${{ github.sha }}
          path: terraform/tfplan
          retention-days: 5

  deploy:
    name: Deploy to EC2
    runs-on: ubuntu-latest
    needs: [build, terraform-plan]
    environment: ${{ github.event.inputs.environment || 'staging' }}
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master' || github.event_name == 'workflow_dispatch'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Download Terraform Plan
        uses: actions/download-artifact@v4
        with:
          name: terraform-plan-${{ github.sha }}
          path: terraform/

      - name: Terraform Init
        run: terraform init
        working-directory: ./terraform
        env:
          TF_VAR_environment: ${{ github.event.inputs.environment || 'staging' }}

      - name: Terraform Apply
        run: terraform apply -auto-approve tfplan
        working-directory: ./terraform
        env:
          TF_VAR_key_name: ${{ secrets.EC2_KEY_NAME }}
          TF_VAR_allowed_cidr_blocks: ${{ secrets.ALLOWED_CIDR_BLOCKS }}
          TF_VAR_new_relic_license_key: ${{ secrets.NEW_RELIC_LICENSE_KEY }}
          TF_VAR_db_name: ${{ secrets.DB_NAME || 'foodme' }}
          TF_VAR_db_user: ${{ secrets.DB_USER || 'foodme_user' }}
          TF_VAR_db_password: ${{ secrets.DB_PASSWORD || 'foodme_secure_password_2025!' }}
          TF_VAR_db_port: ${{ secrets.DB_PORT || '5432' }}

      - name: Get EC2 instance details
        id: instance
        run: |
          INSTANCE_IP=$(terraform output -raw instance_public_ip)
          INSTANCE_ID=$(terraform output -raw instance_id)
          echo "ip=$INSTANCE_IP" >> $GITHUB_OUTPUT
          echo "id=$INSTANCE_ID" >> $GITHUB_OUTPUT
        working-directory: ./terraform

      - name: Download dist artifact
        uses: actions/download-artifact@v4
        with:
          name: foodme-dist-${{ github.sha }}
          path: ./dist-download
      - name: Setup SSH key
        if: steps.check_ssh.outputs.ssh_available == 'true'
        run: |
          # Create SSH directory
          mkdir -p ~/.ssh

          # Create private key file with proper formatting
          echo "${{ secrets.EC2_PRIVATE_KEY }}" > ~/.ssh/id_rsa

          # Verify the key was written correctly
          if [ ! -s ~/.ssh/id_rsa ]; then
            echo "❌ Failed to create SSH private key file"
            exit 1
          fi

          # Check key format
          if ! grep -q "BEGIN.*PRIVATE KEY" ~/.ssh/id_rsa; then
            echo "❌ Private key does not appear to be in correct format"
            echo "Expected format should start with '-----BEGIN PRIVATE KEY-----' or '-----BEGIN RSA PRIVATE KEY-----'"
            exit 1
          fi

          if ! grep -q "END.*PRIVATE KEY" ~/.ssh/id_rsa; then
            echo "❌ Private key does not appear to be complete"
            echo "Expected format should end with '-----END PRIVATE KEY-----' or '-----END RSA PRIVATE KEY-----'"
            exit 1
          fi

          # Set correct permissions
          chmod 600 ~/.ssh/id_rsa

          # Verify key file with ssh-keygen (with better error handling)
          if ! ssh-keygen -l -f ~/.ssh/id_rsa > /dev/null 2>&1; then
            echo "❌ SSH private key appears to be invalid or corrupted"
            echo "Key file size: $(wc -c < ~/.ssh/id_rsa) bytes"
            echo "Key file lines: $(wc -l < ~/.ssh/id_rsa) lines"
            exit 1
          fi

          echo "✅ SSH key setup completed"
          echo "Key fingerprint: $(ssh-keygen -l -f ~/.ssh/id_rsa | cut -d' ' -f2)"

      - name: Wait for instance to be ready
        if: steps.check_ssh.outputs.ssh_available == 'true'
        run: |
          echo "Waiting for instance to be ready..."
          echo "Instance IP: ${{ steps.instance.outputs.ip }}"

          # Wait for SSH port to be open
          for i in {1..60}; do
            if nc -z ${{ steps.instance.outputs.ip }} 22 2>/dev/null; then
              echo "✅ SSH port is open"
              break
            fi
            echo "⏳ Waiting for SSH (attempt $i/60)..."
            sleep 5
          done

          # Additional wait for services to start
          echo "Waiting additional 30 seconds for services to start..."
          sleep 30

      - name: Monitor cloud-init and user_data execution
        if: steps.check_ssh.outputs.ssh_available == 'true'
        timeout-minutes: 15
        continue-on-error: true
        run: |
          echo "📋 Monitoring user_data.sh script execution..."
          SSH_OPTS="-i ~/.ssh/id_rsa -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ConnectTimeout=10 -o ServerAliveInterval=10 -o ServerAliveCountMax=3"

          # Function to safely execute SSH commands with timeout
          ssh_exec() {
            timeout 60 ssh $SSH_OPTS ec2-user@${{ steps.instance.outputs.ip }} "$1" 2>/dev/null || echo "Command failed, timed out, or service not ready"
          }

          # Check if SSH is actually working first
          echo "🔍 Testing SSH connectivity..."
          if ! timeout 30 ssh $SSH_OPTS ec2-user@${{ steps.instance.outputs.ip }} "echo 'SSH connected successfully'" 2>/dev/null; then
            echo "❌ SSH connection failed - cannot monitor cloud-init remotely"
            echo "💡 This could mean:"
            echo "   - Instance is still booting"
            echo "   - SSH key is incorrect"
            echo "   - Security group doesn't allow SSH"
            echo "   - Instance failed to start"
            echo ""
            echo "🔄 Will continue deployment and check health endpoint instead..."
            exit 0
          fi
          echo "✅ SSH connection successful"

          # Check cloud-init status with shorter timeout
          echo "🔍 Checking cloud-init status..."
          if ssh_exec "cloud-init status --wait --long" | grep -q "done"; then
            echo "✅ Cloud-init completed successfully"
          else
            echo "⏳ Cloud-init still running or not responding, checking progress..."
            
            # Show last 30 lines of cloud-init output
            echo "📜 Recent cloud-init output:"
            ssh_exec "sudo tail -n 30 /var/log/cloud-init-output.log" || echo "Could not read cloud-init output"
            
            # Check if user_data script is running
            echo "🔍 Checking if user_data script is running..."
            ssh_exec "ps aux | grep -v grep | grep -E '(user.?data|cloud.?init)'" || echo "No user_data processes found"
            
            # Wait a bit more and check again
            echo "⏳ Waiting 60 more seconds for cloud-init to complete..."
            sleep 60
            
            echo "🔍 Final cloud-init status check..."
            ssh_exec "cloud-init status" || echo "Could not get final cloud-init status"
          fi

          # Display cloud-init logs
          echo ""
          echo "📋 ===== CLOUD-INIT OUTPUT LOG (last 100 lines) ====="
          ssh_exec "sudo tail -n 100 /var/log/cloud-init-output.log" || echo "Could not read cloud-init output log"

          echo ""
          echo "📋 ===== CUSTOM USER_DATA EXECUTION LOG ====="
          ssh_exec "sudo cat /var/log/user-data-execution.log 2>/dev/null" || echo "Custom log not found (script may still be running)"

          echo ""
          echo "📋 ===== CLOUD-INIT ERROR LOG (if any) ====="
          ssh_exec "sudo tail -n 20 /var/log/cloud-init.log | grep -i error" || echo "No errors found in cloud-init.log"

          echo ""
          echo "📋 ===== USER DATA SCRIPT SUMMARY ====="
          # Look for our specific markers in the output
          ssh_exec "sudo grep -E '(🚀|✅|❌|⚠️|🏁|🔄 PROGRESS|PostgreSQL|New Relic|Nginx|EC2 setup completed)' /var/log/cloud-init-output.log /var/log/user-data-execution.log 2>/dev/null | tail -40" || echo "Could not find script markers"

          echo ""
          echo "📋 ===== SYSTEM STATUS AFTER USER_DATA ====="
          ssh_exec "sudo systemctl is-active postgresql nginx newrelic-infra | xargs -I {} echo 'Service {}: {}'" || echo "Could not check service status"

          # Check if our config files were created
          echo ""
          echo "📋 ===== CONFIGURATION FILES STATUS ====="
          ssh_exec "ls -la /etc/nginx/conf.d/foodme.conf /etc/newrelic-infra.yml 2>/dev/null | sed 's/^/  /' || echo 'Some config files missing'"

          echo ""
          echo "📋 ===== DOWNLOADED CONFIG FILES ====="
          ssh_exec "ls -la /home/ec2-user/foodme/config/ 2>/dev/null | sed 's/^/  /' || echo 'Config directory not found'"

          echo ""
          echo "✅ Cloud-init monitoring completed"

      - name: Deploy application
        if: steps.check_ssh.outputs.ssh_available == 'true'
        run: |
          # SSH options for better reliability
          SSH_OPTS="-i ~/.ssh/id_rsa -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ConnectTimeout=30 -o ServerAliveInterval=60"

          # Clean up any previous deployment files on server
          ssh $SSH_OPTS ec2-user@${{ steps.instance.outputs.ip }} "sudo rm -rf /tmp/foodme-deploy && mkdir -p /tmp/foodme-deploy"

          # Create deployment directory
          ssh $SSH_OPTS ec2-user@${{ steps.instance.outputs.ip }} "sudo mkdir -p /var/www/foodme"

          # Check what files we have to upload
          echo "📂 Files to upload:"
          if [ -d "./dist-download/dist" ]; then
            echo "✅ Using dist-download/dist/ directory"
            find ./dist-download/dist -name "*.js" -o -name "*.html" -o -name "*.css" | head -10
            # Upload all files from the build artifact
            scp $SSH_OPTS -r ./dist-download/* ec2-user@${{ steps.instance.outputs.ip }}:/tmp/foodme-deploy/
          elif [ -d "./dist-download" ] && [ "$(find ./dist-download -name "*.js" -o -name "*.html" -o -name "*.css" | head -1)" ]; then
            echo "✅ Using dist-download/ directory directly"
            find ./dist-download -name "*.js" -o -name "*.html" -o -name "*.css" | head -10
            # Upload files directly from dist-download
            scp $SSH_OPTS -r ./dist-download/* ec2-user@${{ steps.instance.outputs.ip }}:/tmp/foodme-deploy/
          else
            echo "❌ No web files found to upload"
            echo "Contents of current directory:"
            ls -la
            echo "Contents of dist-download:"
            ls -la ./dist-download/ || echo "dist-download directory not found"
            echo "Searching for any files in dist-download:"
            find ./dist-download -type f | head -20 || echo "No files found"
            exit 1
          fi

          # Move files to web directory and set permissions
          ssh $SSH_OPTS ec2-user@${{ steps.instance.outputs.ip }} "

            # Clear old deployment
            sudo rm -rf /var/www/foodme/dist /var/www/foodme/server /var/www/foodme/package*.json
            
            # Copy new files - ensure all application files are copied
            sudo cp -r /tmp/foodme-deploy/* /var/www/foodme/
            
            # Check if server directory exists in the deployed files
            if [ -d '/var/www/foodme/dist/server' ]; then
              echo '✅ Server directory found in dist/server'
            else
              echo '❌ No server directory found! Listing contents:'
              find /var/www/foodme -type d -name '*server*' || echo 'No server directories found'
              ls -la /var/www/foodme/
              if [ -d '/var/www/foodme/dist' ]; then
                echo 'Contents of dist directory:'
                ls -la /var/www/foodme/dist/
              fi
            fi
            
            # Check if package.json exists
            if [ ! -f '/var/www/foodme/dist/server/package.json' ]; then
              echo '❌ package.json not found in /var/www/foodme/dist/server!'
              ls -la /var/www/foodme/dist/server
            else
              echo '✅ package.json found'
            fi
            
            # Set proper permissions
            sudo chown -R ec2-user:ec2-user /var/www/foodme
            sudo chmod -R 755 /var/www/foodme
            
            # Run the deployment script
            if [ -x '/home/ec2-user/foodme/config/deploy.sh' ]; then
              echo 'Running deployment script...'
              cd /home/ec2-user/foodme/config && ./deploy.sh
            else
              echo 'No deployment script found, manually starting services...'
              
              # Install Node.js dependencies in server directory
              if [ -d '/var/www/foodme/dist/server' ] && [ -f '/var/www/foodme/server/package.json' ]; then
                echo 'Installing Node.js dependencies in server directory...'
                cd /var/www/foodme/server && npm install --omit=dev --unsafe-perm --verbose
              else
                echo 'No package.json found - dependencies may have been installed during build'
              fi

              # Configure Systemd service
              sudo wget -O - "https://raw.githubusercontent.com/mcaronnewrelic/Foodme-NRU/main/terraform/configs/foodme.service" | sudo tee "/etc/systemd/system/foodme.service" > /dev/null
              sudo systemctl enable foodme
              sudo systemctl restart foodme

              # Start services
              sudo systemctl start foodme
              
              # Verify environment variables are available to the service
              echo 'Checking if New Relic environment is accessible to the service:'
              sudo systemctl show foodme --property=Environment | grep NEW_RELIC || echo 'NEW_RELIC not found in service environment'
              
              # Check service status
              echo 'Service status:'
              sudo systemctl is-active foodme || echo 'foodme failed to start'
              
              # Show detailed status if foodme failed
              if ! sudo systemctl is-active foodme >/dev/null; then
                echo 'Foodme service failed - showing detailed status:'
                sudo systemctl status foodme --no-pager || true
                echo 'Recent logs:'
                sudo journalctl -u foodme -n 20 --no-pager || true
                
                # Additional debugging for server directory
                echo 'Checking server directory structure:'
                ls -la /var/www/foodme/dist/server/ || echo 'Server directory not found'
                echo 'Checking package.json in server:'
                ls -la /var/www/foodme/dist/server/package.json || echo 'package.json not found in server'
              fi
            fi
            
            # Clean up temp files
            rm -rf /tmp/foodme-deploy
          "
      - name: Set Release Version from Tag
        run: echo "RELEASE_VERSION=${{ github.ref_name }}" >> $GITHUB_ENV
      - name: New Relic Application Deployment Marker
        uses: newrelic/deployment-marker-action@v2.5.1
        with:
          guid: ${{ secrets.NEW_RELIC_APP_ID }}
          apiKey: ${{ secrets.NEW_RELIC_API_KEY }}
          region: "US"
          user: ${{ github.actor }}
          commit: ${{ github.sha }}
          changelog: ${{ github.event.head_commit.message }}
          version: "${{ env.RELEASE_VERSION }}"
          description: "Deployed FoodMe-${{ github.event.inputs.environment || 'staging' }} by ${{ github.actor }} to ${{ github.event.inputs.environment || 'staging' }}"

      - name: Deployment summary
        run: |
          echo "## 🚀 Deployment Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Environment**: ${{ github.event.inputs.environment || 'staging' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Instance ID**: ${{ steps.instance.outputs.id }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Instance IP**: ${{ steps.instance.outputs.ip }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Application URL**: http://${{ steps.instance.outputs.ip }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Deployed by**: ${{ github.actor }}" >> $GITHUB_STEP_SUMMARY

          # Add health check status
          if [ "${{ steps.health_check.outputs.health_passed }}" == "true" ]; then
            echo "- **Health Check**: ✅ Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "- **Health Check**: ⚠️ Failed (check logs for details)" >> $GITHUB_STEP_SUMMARY
          fi

  cleanup-on-failure:
    name: Cleanup AWS Resources on Failure
    runs-on: ubuntu-latest
    needs: [deploy]
    if: failure() && needs.deploy.result == 'failure'
    environment: ${{ github.event.inputs.environment || 'staging' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Terraform Init
        run: terraform init
        working-directory: ./terraform
        continue-on-error: true

      - name: Terraform Destroy
        run: |
          echo "🧹 Cleaning up AWS resources due to deployment failure..."
          terraform destroy -auto-approve \
            -var="environment=${{ github.event.inputs.environment || 'staging' }}" \
            -var="app_version=${{ github.sha }}" || echo "⚠️ Some resources may have failed to destroy"
        working-directory: ./terraform
        env:
          TF_VAR_key_name: ${{ secrets.EC2_KEY_NAME }}
          TF_VAR_allowed_cidr_blocks: ${{ secrets.ALLOWED_CIDR_BLOCKS }}
        continue-on-error: true

      - name: Cleanup summary
        run: |
          echo "## 🧹 Cleanup Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Reason**: Deployment failed, cleaned up AWS resources" >> $GITHUB_STEP_SUMMARY
          echo "- **Environment**: ${{ github.event.inputs.environment || 'staging' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Action**: Terraform destroy executed" >> $GITHUB_STEP_SUMMARY
          echo "- **Note**: Please verify in AWS console that all resources were removed" >> $GITHUB_STEP_SUMMARY

  cleanup-artifacts:
    name: Cleanup Build Artifacts
    runs-on: ubuntu-latest
    needs: [deploy, cleanup-on-failure]
    if: always()

    steps:
      - name: Delete artifacts
        uses: actions/github-script@v7
        with:
          script: |
            try {
              const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
                owner: context.repo.owner,
                repo: context.repo.repo,
                run_id: context.runId,
              });

              let deletedCount = 0;
              let failedCount = 0;

              for (const artifact of artifacts.data.artifacts) {
                if (artifact.name.includes('${{ github.sha }}')) {
                  try {
                    await github.rest.actions.deleteArtifact({
                      owner: context.repo.owner,
                      repo: context.repo.repo,
                      artifact_id: artifact.id,
                    });
                    console.log(`✅ Deleted artifact: ${artifact.name}`);
                    deletedCount++;
                  } catch (error) {
                    console.log(`⚠️ Failed to delete artifact ${artifact.name}: ${error.message}`);
                    failedCount++;
                  }
                }
              }

              console.log(`\n📊 Cleanup Summary:`);
              console.log(`- Artifacts deleted: ${deletedCount}`);
              console.log(`- Deletion failures: ${failedCount}`);
              
              if (failedCount > 0) {
                console.log(`\n💡 Note: Some artifacts may be automatically cleaned up by GitHub after the retention period.`);
                console.log(`This error is usually due to permission restrictions and won't affect your application.`);
              }
            } catch (error) {
              console.log(`⚠️ Unable to cleanup artifacts: ${error.message}`);
              console.log(`This is usually due to GitHub token permissions and won't affect your application.`);
              console.log(`Artifacts will be automatically cleaned up by GitHub after the retention period.`);
            }
        continue-on-error: true
