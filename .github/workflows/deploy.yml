name: Deploy to EC2 with Terraform

"on":
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]
  workflow_dispatch:
    inputs:
      environment:
        description: "Environment to deploy to"
        required: true
        default: "staging"
        type: choice
        options:
          - staging
          - production

# Add explicit permissions for the workflow
permissions:
  contents: read
  actions: write
  deployments: write
  id-token: write

env:
  AWS_REGION: us-west-2
  TF_VERSION: 1.6.0
  NODE_VERSION: 22

jobs:
  build:
    name: Build Application
    runs-on: ubuntu-latest
    outputs:
      dist-artifact: ${{ steps.upload.outputs.artifact-id }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"

      - name: Install dependencies
        run: npm run install:all

      - name: Build Angular app
        run: npm run build:angular

      - name: Build complete application
        run: npm run build

      - name: List dist contents
        run: |
          echo "Contents of dist directory:"
          ls -la dist/
          find dist/ -type f -name "*.js" -o -name "*.html" -o -name "*.css" | head -20

      - name: Upload dist artifact
        id: upload
        uses: actions/upload-artifact@v4
        with:
          name: foodme-dist-${{ github.sha }}
          path: |
            dist/
            !dist/**/*.map
            !dist/**/test/**
          retention-days: 30

  terraform-plan:
    name: Terraform Plan
    runs-on: ubuntu-latest
    needs: build
    environment: ${{ github.event.inputs.environment || 'staging' }}
    outputs:
      tfplan: ${{ steps.plan.outputs.tfplan }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Terraform Format Check
        run: terraform fmt -check
        working-directory: ./terraform

      - name: Terraform Init
        run: terraform init
        working-directory: ./terraform
        env:
          TF_VAR_environment: ${{ github.event.inputs.environment || 'staging' }}

      - name: Terraform Validate
        run: terraform validate
        working-directory: ./terraform

      - name: Terraform Plan
        id: plan
        run: |
          terraform plan \
            -var="environment=${{ github.event.inputs.environment || 'staging' }}" \
            -var="app_version=${{ github.sha }}" \
            -out=tfplan
          echo "tfplan=tfplan" >> $GITHUB_OUTPUT
        working-directory: ./terraform
        env:
          TF_VAR_key_name: ${{ secrets.EC2_KEY_NAME }}
          TF_VAR_allowed_cidr_blocks: ${{ secrets.ALLOWED_CIDR_BLOCKS }}
          TF_VAR_new_relic_license_key: ${{ secrets.NEW_RELIC_LICENSE_KEY }}
          TF_VAR_db_name: ${{ secrets.DB_NAME }}
          TF_VAR_db_user: ${{ secrets.DB_USER }}
          TF_VAR_db_password: ${{ secrets.DB_PASSWORD }}
          TF_VAR_db_port: ${{ secrets.DB_PORT }}

      - name: Upload Terraform Plan
        uses: actions/upload-artifact@v4
        with:
          name: terraform-plan-${{ github.sha }}
          path: terraform/tfplan
          retention-days: 5

  deploy:
    name: Deploy to EC2
    runs-on: ubuntu-latest
    needs: [build, terraform-plan]
    environment: ${{ github.event.inputs.environment || 'staging' }}
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master' || github.event_name == 'workflow_dispatch'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Download Terraform Plan
        uses: actions/download-artifact@v4
        with:
          name: terraform-plan-${{ github.sha }}
          path: terraform/

      - name: Terraform Init
        run: terraform init
        working-directory: ./terraform
        env:
          TF_VAR_environment: ${{ github.event.inputs.environment || 'staging' }}

      - name: Terraform Apply
        run: terraform apply -auto-approve tfplan
        working-directory: ./terraform
        env:
          TF_VAR_key_name: ${{ secrets.EC2_KEY_NAME }}
          TF_VAR_allowed_cidr_blocks: ${{ secrets.ALLOWED_CIDR_BLOCKS }}
          TF_VAR_new_relic_license_key: ${{ secrets.NEW_RELIC_LICENSE_KEY }}
          TF_VAR_db_name: ${{ secrets.DB_NAME }}
          TF_VAR_db_user: ${{ secrets.DB_USER }}
          TF_VAR_db_password: ${{ secrets.DB_PASSWORD }}
          TF_VAR_db_port: ${{ secrets.DB_PORT }}

      - name: Get EC2 instance details
        id: instance
        run: |
          INSTANCE_IP=$(terraform output -raw instance_public_ip)
          INSTANCE_ID=$(terraform output -raw instance_id)
          echo "ip=$INSTANCE_IP" >> $GITHUB_OUTPUT
          echo "id=$INSTANCE_ID" >> $GITHUB_OUTPUT
        working-directory: ./terraform

      - name: Download dist artifact
        uses: actions/download-artifact@v4
        with:
          name: foodme-dist-${{ github.sha }}
          path: ./dist-download

      - name: List downloaded files for debugging
        run: |
          echo "üìÇ Contents of dist-download directory:"
          find ./dist-download -type f | head -20
          echo ""
          echo "üìÅ Directory structure:"
          ls -la ./dist-download/

      - name: Check SSH key availability
        id: check_ssh
        run: |
          if [ -n "${{ secrets.EC2_PRIVATE_KEY }}" ]; then
            echo "ssh_available=true" >> $GITHUB_OUTPUT
          else
            echo "ssh_available=false" >> $GITHUB_OUTPUT
          fi

      - name: Setup SSH key
        if: steps.check_ssh.outputs.ssh_available == 'true'
        run: |
          # Create SSH directory
          mkdir -p ~/.ssh

          # Create private key file with proper formatting
          echo "${{ secrets.EC2_PRIVATE_KEY }}" > ~/.ssh/id_rsa

          # Verify the key was written correctly
          if [ ! -s ~/.ssh/id_rsa ]; then
            echo "‚ùå Failed to create SSH private key file"
            exit 1
          fi

          # Check key format
          if ! grep -q "BEGIN.*PRIVATE KEY" ~/.ssh/id_rsa; then
            echo "‚ùå Private key does not appear to be in correct format"
            echo "Expected format should start with '-----BEGIN PRIVATE KEY-----' or '-----BEGIN RSA PRIVATE KEY-----'"
            exit 1
          fi

          if ! grep -q "END.*PRIVATE KEY" ~/.ssh/id_rsa; then
            echo "‚ùå Private key does not appear to be complete"
            echo "Expected format should end with '-----END PRIVATE KEY-----' or '-----END RSA PRIVATE KEY-----'"
            exit 1
          fi

          # Set correct permissions
          chmod 600 ~/.ssh/id_rsa

          # Verify key file with ssh-keygen (with better error handling)
          if ! ssh-keygen -l -f ~/.ssh/id_rsa > /dev/null 2>&1; then
            echo "‚ùå SSH private key appears to be invalid or corrupted"
            echo "Key file size: $(wc -c < ~/.ssh/id_rsa) bytes"
            echo "Key file lines: $(wc -l < ~/.ssh/id_rsa) lines"
            exit 1
          fi

          echo "‚úÖ SSH key setup completed"
          echo "Key fingerprint: $(ssh-keygen -l -f ~/.ssh/id_rsa | cut -d' ' -f2)"

      - name: Wait for instance to be ready
        if: steps.check_ssh.outputs.ssh_available == 'true'
        run: |
          echo "Waiting for instance to be ready..."
          echo "Instance IP: ${{ steps.instance.outputs.ip }}"

          # Wait for SSH port to be open
          for i in {1..60}; do
            if nc -z ${{ steps.instance.outputs.ip }} 22 2>/dev/null; then
              echo "‚úÖ SSH port is open"
              break
            fi
            echo "‚è≥ Waiting for SSH (attempt $i/60)..."
            sleep 5
          done

          # Additional wait for services to start
          echo "Waiting additional 30 seconds for services to start..."
          sleep 30

      - name: Deploy application
        if: steps.check_ssh.outputs.ssh_available == 'true'
        run: |
          # SSH options for better reliability
          SSH_OPTS="-i ~/.ssh/id_rsa -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ConnectTimeout=30 -o ServerAliveInterval=60"

          # Clean up any previous deployment files on server
          ssh $SSH_OPTS ec2-user@${{ steps.instance.outputs.ip }} "sudo rm -rf /tmp/foodme-deploy && mkdir -p /tmp/foodme-deploy"

          # Create deployment directory
          ssh $SSH_OPTS ec2-user@${{ steps.instance.outputs.ip }} "sudo mkdir -p /var/www/foodme"

          # Check what files we have to upload
          echo "üìÇ Files to upload:"
          if [ -d "./dist-download/dist" ]; then
            echo "‚úÖ Using dist-download/dist/ directory"
            find ./dist-download/dist -name "*.js" -o -name "*.html" -o -name "*.css" | head -10
            # Upload dist files from the correct path
            scp $SSH_OPTS -r ./dist-download/dist/* ec2-user@${{ steps.instance.outputs.ip }}:/tmp/foodme-deploy/
          elif [ -d "./dist-download" ] && [ "$(find ./dist-download -name "*.js" -o -name "*.html" -o -name "*.css" | head -1)" ]; then
            echo "‚úÖ Using dist-download/ directory directly"
            find ./dist-download -name "*.js" -o -name "*.html" -o -name "*.css" | head -10
            # Upload files directly from dist-download
            scp $SSH_OPTS -r ./dist-download/* ec2-user@${{ steps.instance.outputs.ip }}:/tmp/foodme-deploy/
          else
            echo "‚ùå No web files found to upload"
            echo "Contents of current directory:"
            ls -la
            echo "Contents of dist-download:"
            ls -la ./dist-download/ || echo "dist-download directory not found"
            echo "Searching for any files in dist-download:"
            find ./dist-download -type f | head -20 || echo "No files found"
            exit 1
          fi

          # Upload database initialization files
          echo "üìÇ Uploading database initialization files..."
          if [ -d "./db" ]; then
            scp $SSH_OPTS -r ./db ec2-user@${{ steps.instance.outputs.ip }}:/tmp/foodme-deploy/
            echo "‚úÖ Database files uploaded"
          else
            echo "‚ö†Ô∏è No database directory found"
          fi

          # Move files to web directory and set permissions
          ssh $SSH_OPTS ec2-user@${{ steps.instance.outputs.ip }} "
            # Stop services first
            sudo systemctl stop foodme || true
            sudo systemctl stop nginx || true
            
            # Clear old deployment
            sudo rm -rf /var/www/foodme/*
            
            # Copy new files
            sudo cp -r /tmp/foodme-deploy/* /var/www/foodme/
            
            # Set proper permissions
            sudo chown -R nginx:nginx /var/www/foodme
            sudo chmod -R 755 /var/www/foodme
            
            # Create logs directory if it doesn't exist
            sudo mkdir -p /var/log/foodme
            sudo chown ec2-user:ec2-user /var/log/foodme
            
            # Start services
            sudo systemctl start nginx
            sudo systemctl start foodme
            
            # Check service status
            echo 'Service status:'
            sudo systemctl is-active nginx || echo 'nginx failed to start'
            sudo systemctl is-active foodme || echo 'foodme failed to start'
            
            # Clean up temp files
            rm -rf /tmp/foodme-deploy
          "

      - name: Alternative deployment notice
        if: steps.check_ssh.outputs.ssh_available == 'false'
        run: |
          echo "üîÑ SSH deployment skipped - no EC2_PRIVATE_KEY provided"
          echo "üìù To enable SSH deployment:"
          echo "   1. Run: cd terraform && ./create-keypair.sh"
          echo "   2. Add EC2_KEY_NAME and EC2_PRIVATE_KEY secrets to GitHub"
          echo "   3. Re-run the workflow"
          echo ""
          echo "üí° Alternative: Use AWS Systems Manager Session Manager for deployment"
          echo "   This would require modifying the deployment strategy"

      - name: Health check
        id: health_check
        run: |
          echo "Performing health check..."
          echo "Instance IP: ${{ steps.instance.outputs.ip }}"

          # Wait a bit for services to start if SSH deployment was skipped
          if [ "${{ steps.check_ssh.outputs.ssh_available }}" == "false" ]; then
            echo "‚ÑπÔ∏è  SSH deployment was skipped, waiting longer for user_data script to complete..."
            echo "‚è≥ Waiting 3 minutes for user_data script and services to start..."
            sleep 180
          else
            echo "‚ÑπÔ∏è  SSH deployment completed, waiting for services to stabilize..."
            sleep 60
          fi

          # First, check if the instance is reachable
          echo "üîç Testing basic connectivity..."
          if ! curl -s --connect-timeout 10 http://${{ steps.instance.outputs.ip }} > /dev/null; then
            echo "‚ùå Cannot connect to instance - checking if it's still starting up..."
          fi

          # Check different endpoints to diagnose the issue
          echo "üîç Performing comprehensive health checks..."

          # Track consecutive 502 errors for targeted intervention
          consecutive_502_count=0

          for i in {1..15}; do
            echo "‚è≥ Health check attempt $i/15..."
            
            # Try the health endpoint first
            HEALTH_CODE=$(curl -s -o /dev/null -w "%{http_code}" --connect-timeout 10 --max-time 30 http://${{ steps.instance.outputs.ip }}/health 2>/dev/null || echo "000")
            
            if [ "$HEALTH_CODE" = "200" ]; then
              echo "‚úÖ Health check passed on /health endpoint"
              echo "health_passed=true" >> $GITHUB_OUTPUT
              exit 0
            fi
            
            # Try the root endpoint as fallback
            ROOT_CODE=$(curl -s -o /dev/null -w "%{http_code}" --connect-timeout 10 --max-time 30 http://${{ steps.instance.outputs.ip }}/ 2>/dev/null || echo "000")
            
            if [ "$ROOT_CODE" = "200" ]; then
              echo "‚úÖ Root endpoint accessible - application seems to be running"
              echo "health_passed=true" >> $GITHUB_OUTPUT
              exit 0
            fi
            
            # Analyze the error codes
            echo "üìä Health endpoint: HTTP $HEALTH_CODE, Root endpoint: HTTP $ROOT_CODE"
            
            # Handle specific error scenarios
            if [ "$HEALTH_CODE" = "502" ] || [ "$ROOT_CODE" = "502" ]; then
              consecutive_502_count=$((consecutive_502_count + 1))
              echo "üîç 502 Bad Gateway (count: $consecutive_502_count) - nginx running but backend not responding"
              
              # If we've seen multiple 502s and have SSH access, try to restart the service
              if [ $consecutive_502_count -ge 3 ] && [ "${{ steps.check_ssh.outputs.ssh_available }}" == "true" ] && [ $i -le 10 ]; then
                echo "üîß Attempting to restart backend service via SSH..."
                
                # Create a temporary script for service restart
                {
                  echo '#!/bin/bash'
                  echo 'echo "Checking service status..."'
                  echo 'sudo systemctl status foodme --no-pager || true'
                  echo ''
                  echo 'echo "Checking if Node.js process is running..."'
                  echo 'ps aux | grep node || true'
                  echo ''
                  echo 'echo "Checking if port 3000 is in use..."'
                  echo 'sudo netstat -tlnp | grep :3000 || true'
                  echo ''
                  echo 'echo "Restarting foodme service..."'
                  echo 'sudo systemctl restart foodme'
                  echo ''
                  echo 'echo "Waiting for service to start..."'
                  echo 'sleep 10'
                  echo ''
                  echo 'echo "Checking service status after restart..."'
                  echo 'sudo systemctl status foodme --no-pager || true'
                  echo ''
                  echo 'echo "Testing local health endpoint..."'
                  echo 'curl -f http://localhost:3000/health || curl -s http://localhost:3000/health || echo "Local health check failed"'
                } > restart_service.sh

                # Execute the restart script
                if scp -i /tmp/ssh_key -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null restart_service.sh ec2-user@${{ steps.instance.outputs.ip }}:/tmp/restart_service.sh; then
                  ssh -i /tmp/ssh_key -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ec2-user@${{ steps.instance.outputs.ip }} "chmod +x /tmp/restart_service.sh && /tmp/restart_service.sh" || true
                  echo "üîß Service restart attempted, waiting 30 seconds..."
                  sleep 30
                  consecutive_502_count=0  # Reset counter after intervention
                  continue
                else
                  echo "‚ö†Ô∏è  Could not restart service via SSH"
                fi
              fi
              
            elif [ "$HEALTH_CODE" = "503" ] || [ "$ROOT_CODE" = "503" ]; then
              echo "üîç 503 Service Unavailable - service temporarily unavailable"
              consecutive_502_count=0  # Reset 502 counter
              
            elif [ "$HEALTH_CODE" = "000" ] && [ "$ROOT_CODE" = "000" ]; then
              echo "üîç Connection failed - instance may still be starting or network issue"
              consecutive_502_count=0  # Reset 502 counter
              
            elif [ "$HEALTH_CODE" = "404" ] || [ "$ROOT_CODE" = "404" ]; then
              echo "üîç 404 Not Found - nginx running but routes not configured properly"
              consecutive_502_count=0  # Reset 502 counter
              
            else
              echo "üîç Unexpected response codes"
              consecutive_502_count=0  # Reset 502 counter
            fi
            
            # Provide specific guidance based on error pattern
            if [ $consecutive_502_count -ge 2 ]; then
              echo "üîç Consistent 502 errors suggest backend service issues:"
              echo "   - Node.js app may have crashed or failed to start"
              echo "   - Dependencies might be missing"
              echo "   - Port 3000 may not be listening"
            fi
            
            # Wait before next attempt (longer wait for 502 errors)
            if [ $i -lt 15 ]; then
              if [ "$HEALTH_CODE" = "502" ] || [ "$ROOT_CODE" = "502" ]; then
                echo "‚è≥ 502 error detected, waiting 45 seconds for potential auto-recovery..."
                sleep 45
              else
                echo "‚è≥ Waiting 30 seconds before next attempt..."
                sleep 30
              fi
            fi
          done

          echo "‚ùå Health check failed after 15 attempts"
          echo "health_passed=false" >> $GITHUB_OUTPUT

          # Final diagnostic information
          echo ""
          echo "ÔøΩ Final diagnostic check:"
          echo "Instance IP: ${{ steps.instance.outputs.ip }}"

          # Check if nginx is responding at all
          if curl -s --connect-timeout 5 http://${{ steps.instance.outputs.ip }} > /dev/null 2>&1; then
            echo "‚úÖ Nginx is responding"
          else
            echo "‚ùå Cannot connect to nginx"
          fi

          echo ""
          echo "üí° Troubleshooting steps:"
          echo "1. SSH into the instance: ssh -i your-key.pem ec2-user@${{ steps.instance.outputs.ip }}"
          echo "2. Check service status: sudo systemctl status nginx foodme"
          echo "3. Check logs: sudo journalctl -u foodme -n 50"
          echo "4. Check nginx logs: sudo tail -f /var/log/nginx/error.log"
          echo "5. Check if app is listening: sudo netstat -tlnp | grep :3000"

          exit 1

      - name: Deployment failure cleanup
        if: failure() && steps.health_check.outputs.health_passed == 'false'
        run: |
          echo "üö® Deployment failed during health check. Consider cleaning up AWS resources."
          echo "health_check_failed=true" >> $GITHUB_ENV
        continue-on-error: true

      - name: Send New Relic deployment marker
        if: success()
        run: |
          USER="${{ github.actor }}"
          CHANGELOG="${{ github.event.head_commit.message }}"
          COMMIT="${{ github.sha }}"

          curl -X POST https://api.newrelic.com/graphql \
            -H 'Content-Type: application/json' \
            -H 'API-Key: ${{ secrets.NEW_RELIC_API_KEY }}' \
            --data-raw '{
              "query": "mutation { 
                changeTrackingCreateDeployment(deployment: {
                  description: \"FoodMe EC2 Deployment via GitHub Actions\",
                  user: \"'$USER'\",
                  changelog: \"'$CHANGELOG'\",
                  commit: \"'$COMMIT'\",
                  deploymentType: BASIC
                }) { 
                  deploymentId 
                } 
              }"
            }'
        continue-on-error: true

      - name: Deployment summary
        run: |
          echo "## üöÄ Deployment Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Environment**: ${{ github.event.inputs.environment || 'staging' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Instance ID**: ${{ steps.instance.outputs.id }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Instance IP**: ${{ steps.instance.outputs.ip }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Application URL**: http://${{ steps.instance.outputs.ip }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Deployed by**: ${{ github.actor }}" >> $GITHUB_STEP_SUMMARY

  cleanup-on-failure:
    name: Cleanup AWS Resources on Failure
    runs-on: ubuntu-latest
    needs: [deploy]
    if: failure() && needs.deploy.result == 'failure'
    environment: ${{ github.event.inputs.environment || 'staging' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Terraform Init
        run: terraform init
        working-directory: ./terraform
        continue-on-error: true

      - name: Terraform Destroy
        run: |
          echo "üßπ Cleaning up AWS resources due to deployment failure..."
          terraform destroy -auto-approve \
            -var="environment=${{ github.event.inputs.environment || 'staging' }}" \
            -var="app_version=${{ github.sha }}" || echo "‚ö†Ô∏è Some resources may have failed to destroy"
        working-directory: ./terraform
        env:
          TF_VAR_key_name: ${{ secrets.EC2_KEY_NAME }}
          TF_VAR_allowed_cidr_blocks: ${{ secrets.ALLOWED_CIDR_BLOCKS }}
        continue-on-error: true

      - name: Cleanup summary
        run: |
          echo "## üßπ Cleanup Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Reason**: Deployment failed, cleaned up AWS resources" >> $GITHUB_STEP_SUMMARY
          echo "- **Environment**: ${{ github.event.inputs.environment || 'staging' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Action**: Terraform destroy executed" >> $GITHUB_STEP_SUMMARY
          echo "- **Note**: Please verify in AWS console that all resources were removed" >> $GITHUB_STEP_SUMMARY

  cleanup-artifacts:
    name: Cleanup Build Artifacts
    runs-on: ubuntu-latest
    needs: [deploy, cleanup-on-failure]
    if: always()

    steps:
      - name: Delete artifacts
        uses: actions/github-script@v7
        with:
          script: |
            try {
              const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
                owner: context.repo.owner,
                repo: context.repo.repo,
                run_id: context.runId,
              });

              let deletedCount = 0;
              let failedCount = 0;

              for (const artifact of artifacts.data.artifacts) {
                if (artifact.name.includes('${{ github.sha }}')) {
                  try {
                    await github.rest.actions.deleteArtifact({
                      owner: context.repo.owner,
                      repo: context.repo.repo,
                      artifact_id: artifact.id,
                    });
                    console.log(`‚úÖ Deleted artifact: ${artifact.name}`);
                    deletedCount++;
                  } catch (error) {
                    console.log(`‚ö†Ô∏è Failed to delete artifact ${artifact.name}: ${error.message}`);
                    failedCount++;
                  }
                }
              }

              console.log(`\nüìä Cleanup Summary:`);
              console.log(`- Artifacts deleted: ${deletedCount}`);
              console.log(`- Deletion failures: ${failedCount}`);
              
              if (failedCount > 0) {
                console.log(`\nüí° Note: Some artifacts may be automatically cleaned up by GitHub after the retention period.`);
                console.log(`This error is usually due to permission restrictions and won't affect your application.`);
              }
            } catch (error) {
              console.log(`‚ö†Ô∏è Unable to cleanup artifacts: ${error.message}`);
              console.log(`This is usually due to GitHub token permissions and won't affect your application.`);
              console.log(`Artifacts will be automatically cleaned up by GitHub after the retention period.`);
            }
        continue-on-error: true
